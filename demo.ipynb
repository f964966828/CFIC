{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages & define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# define model & tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda().half()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define article and question\n",
    "- 問題是 ChatGPT 生的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define article & question, target sentence: \"[April 3] Researchers ...\"\n",
    "with open(\"long_knowledge.txt\", \"r\") as file:\n",
    "    article = file.read().replace('\\\\n', '').replace('. ', '.')\n",
    "question = \"What new attack methods used by the APT41 subgroup Earth Freybug were discovered, and how do they target endpoint protection systems?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ sentence 1, length: 3543 ------\n",
      "This week, the most significant news in the IT and cybersecurity world is the accidental discovery of a backdoor planted in XZ/liblzma.Red Hat also announced a related vulnerability, CVE-2024--3094, w ...\n",
      "------ sentence 2, length: 783 ------\n",
      "[April 1] A supply chain attack that had been lurking for three years was discovered, with the XZ Utils library recently implanted with a backdoor.The revelation of this supply chain attack over the w ...\n",
      "------ sentence 3, length: 837 ------\n",
      "[April 2] CISA reported to the US government about an Ivanti system intrusion incident.At the beginning of this year, Ivanti announced a series of Connect Secure and Policy Secure vulnerabilities, onc ...\n",
      "------ sentence 4, length: 688 ------\n",
      "[April 3] Researchers discovered that a group under the Chinese hacker organization APT41 is using more covert methods to evade detection.In recent years, attack activities by the Chinese hacker organ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/nycu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# split article into sentences using nltk\n",
    "nltk.download('punkt_tab')\n",
    "sentences = nltk.tokenize.sent_tokenize(article)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"------ sentence {i + 1}, length: {len(sentence)} ------\")\n",
    "    print(sentence[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get prompt and apply template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([1, 1186])\n",
      "sentence_ids shape: [torch.Size([1, 667]), torch.Size([1, 154]), torch.Size([1, 144]), torch.Size([1, 128])]\n"
     ]
    }
   ],
   "source": [
    "# get prompt and apply template\n",
    "prompt = f\"\"\"\\\n",
    "Below is an article, read the article and answer my question after the article.\n",
    "Now the article begins:\n",
    "{article}\n",
    "Now the article ends.\n",
    "Select several sentences from the article to answer my question.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "sentence_ids_list = [tokenizer(sentence, return_tensors=\"pt\").input_ids.cuda() for sentence in sentences]\n",
    "\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"sentence_ids shape:\", [sentence_ids.shape for sentence_ids in sentence_ids_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Constrained Sentence Prefix Decoding\n",
    "- 找到可以區分不同 sentence 為止，所以第一個找到 \"This\" 就停，其他的需要再找到 \"[April 1/2/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probs_list(input_ids, sentence_ids_list):\n",
    "    prefix_map = {}\n",
    "    for i, sentence_ids in enumerate(sentence_ids_list):\n",
    "        prefix = sentence_ids[0][0].item()\n",
    "        if prefix not in prefix_map:\n",
    "            prefix_map[prefix] = []\n",
    "        prefix_map[prefix].append(i)\n",
    "    prefixes = list(prefix_map.keys())\n",
    "    \n",
    "    if len(prefix_map) == 1:\n",
    "        probs_list = [[1] for _ in sentence_ids_list]    \n",
    "    else:\n",
    "        probs_list = [None for _ in sentence_ids_list]  \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, use_cache=True)\n",
    "        \n",
    "        probs = torch.softmax(outputs.logits[0, -1, prefixes], dim=-1)\n",
    "        for prefix, prob in zip(prefixes, probs):\n",
    "            for idx in prefix_map[prefix]:\n",
    "                probs_list[idx] = [prob.item()]\n",
    "    \n",
    "    for prefix, indices in prefix_map.items():\n",
    "        if len(indices) == 1:\n",
    "            continue\n",
    "        \n",
    "        next_input_ids = torch.cat([input_ids, torch.tensor(prefix).reshape(1, 1).cuda()], dim=-1)\n",
    "        next_sentence_ids_list = [sentence_ids_list[idx][:, 1:] for idx in indices]\n",
    "        next_probs_list = calculate_probs_list(next_input_ids, next_sentence_ids_list)\n",
    "        \n",
    "        for idx, probs in zip(indices, next_probs_list):\n",
    "            probs_list[idx].extend(probs)\n",
    "    \n",
    "    return probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ sentence 1 ------\n",
      "prefix: This\n",
      "probs: [0.09808349609375]\n",
      "score: -2.321936162101722\n",
      "------ sentence 2 ------\n",
      "prefix: [April 1\n",
      "probs: [0.90185546875, 1, 1, 0.016510009765625]\n",
      "score: -1.0517723588782142\n",
      "------ sentence 3 ------\n",
      "prefix: [April 2\n",
      "probs: [0.90185546875, 1, 1, 0.00856781005859375]\n",
      "score: -1.2157610301851964\n",
      "------ sentence 4 ------\n",
      "prefix: [April 3\n",
      "probs: [0.90185546875, 1, 1, 0.97509765625]\n",
      "score: -0.03212966467593541\n"
     ]
    }
   ],
   "source": [
    "def constrained_sentence_prefix_decoding(input_ids, sentence_ids_list, top_k=2):\n",
    "    probs_list = calculate_probs_list(input_ids, sentence_ids_list)\n",
    "    scores = [np.mean(np.log(probs)) for probs in probs_list]\n",
    "    indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    for i in range(len(sentence_ids_list)):\n",
    "        print(f\"------ sentence {i + 1} ------\")\n",
    "        print(\"prefix:\", tokenizer.decode(sentence_ids_list[i][0, :len(probs_list[i])]))\n",
    "        print(\"probs:\", probs_list[i])\n",
    "        print(\"score:\", scores[i])\n",
    "    \n",
    "    return [sentence_ids_list[idx] for idx in indices]\n",
    "\n",
    "sentence_ids_list = constrained_sentence_prefix_decoding(input_ids, sentence_ids_list, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Skip Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ evidence sentence 1 ------\n",
      "content: [April 3] Researchers discovered that a group under the Chinese hacker organization APT41 is using m ...\n",
      "idx (in article): 5163\n",
      "length of sub_article (# tokens): 128\n",
      "eos_idx (maximum eos prob): 127\n",
      "------ evidence sentence 2 ------\n",
      "content: [April 1] A supply chain attack that had been lurking for three years was discovered, with the XZ Ut ...\n",
      "idx (in article): 3543\n",
      "length of sub_article (# tokens): 256\n",
      "eos_idx (maximum eos prob): 81\n",
      "------------------------------\n",
      "intervals: [(5163, 5850), (3543, 3975)]\n"
     ]
    }
   ],
   "source": [
    "def find_subarray_index(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    for i in range(n - m + 1):\n",
    "        if a[i : i + m] == b:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def skip_decoding(input_ids, sentence_ids_list, max_length=256):\n",
    "    intervals = []\n",
    "    for i, sentence_ids in enumerate(sentence_ids_list):\n",
    "        sentence = tokenizer.decode(sentence_ids[0], skip_special_token=False)\n",
    "        idx = find_subarray_index(article, sentence)\n",
    "        sub_article_ids = tokenizer(article[idx:], return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids.cuda()\n",
    "        \n",
    "        sentence_input_ids = torch.cat([input_ids, sub_article_ids], dim=-1)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(sentence_input_ids, use_cache=True)\n",
    "        \n",
    "        probs = torch.softmax(outputs.logits[0, -len(sub_article_ids[0]):, :], dim=-1)\n",
    "        eos_idx = torch.argmax(probs[:, tokenizer.eos_token_id])\n",
    "        evidence_sentence = tokenizer.decode(sub_article_ids[0, :eos_idx], skip_special_token=False)        \n",
    "        intervals.append((idx, idx + len(evidence_sentence)))\n",
    "        \n",
    "        print(f\"------ evidence sentence {i + 1} ------\")\n",
    "        print(\"content:\", sentence[:100], \"...\")\n",
    "        print(\"idx (in article):\", idx)\n",
    "        print(\"length of sub_article (# tokens):\", len(sub_article_ids[0]))\n",
    "        print(\"eos_idx (maximum eos prob):\", eos_idx.item())\n",
    "        \n",
    "    return intervals\n",
    "\n",
    "intervals = skip_decoding(input_ids, sentence_ids_list, max_length=256)\n",
    "print('-' * 30)\n",
    "print(\"intervals:\", intervals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Merge evidence sentences\n",
    "- 這個 case 剛好沒有 intersect 所以結果不變\n",
    "- 下面放一些會 merge 的 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervals: [(3543, 3975), (5163, 5850)]\n"
     ]
    }
   ],
   "source": [
    "def merge_interval(intervals):\n",
    "    if not intervals or len(intervals) == 1:\n",
    "        return intervals\n",
    "    \n",
    "    intervals.sort(key=lambda x: x[0])\n",
    "    merged = [intervals[0]]\n",
    "\n",
    "    for current in intervals[1:]:\n",
    "        prev_start, prev_end = merged[-1]\n",
    "        curr_start, curr_end = current\n",
    "\n",
    "        if curr_start <= prev_end:\n",
    "            merged[-1] = (prev_start, max(prev_end, curr_end))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "intervals = merge_interval(intervals)\n",
    "print(\"intervals:\", intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 5), (7, 11)]\n",
      "[(1, 4), (5, 7)]\n",
      "[(1, 2), (3, 4), (5, 6)]\n"
     ]
    }
   ],
   "source": [
    "test_intervals = [(1, 3), (2, 5), (7, 11)]\n",
    "print(merge_interval(test_intervals))\n",
    "\n",
    "test_intervals = [(1, 4), (5, 6), (6, 7)]\n",
    "print(merge_interval(test_intervals))\n",
    "\n",
    "test_intervals = [(1, 2), (3, 4), (5, 6)]\n",
    "print(merge_interval(test_intervals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- high-quality knowledge 1 -------\n",
      "[April 1] A supply chain attack that had been lurking for three years was discovered, with the XZ Utils library recently implanted with a backdoor.The revelation of this supply chain attack over the weekend shocked the entire cybersecurity community and kept the IT world busy with patching.Attackers targeted the XZ Utils data compression library to implant a backdoor, which would allow attackers to bypass the SSHD authentication\n",
      "------- high-quality knowledge 2 -------\n",
      "[April 3] Researchers discovered that a group under the Chinese hacker organization APT41 is using more covert methods to evade detection.In recent years, attack activities by the Chinese hacker organization APT41 have been reported from time to time, but few related incidents were disclosed at the beginning of this year.Recently, researchers discovered the attack methods of a group under this organization.Cybersecurity firm Trend Micro discovered that the group, Earth Freybug, specifically targets endpoint protection systems (note: the researchers probably refer to EDR-related systems) to track malicious actions, attempting to make these systems unable to detect their movements\n"
     ]
    }
   ],
   "source": [
    "# show result\n",
    "for i, (start, end) in enumerate(intervals):\n",
    "    print(f\"------- high-quality knowledge {i + 1} -------\")\n",
    "    print(article[start : end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些測試用問題，都是 ChatGPT 生的，但我記得有些抓出來的 high-quality knowlege 會有錯就是了，分別對應 target sentence 第一二三四個\n",
    "question_1 = \"What is the significance of the backdoor discovered in XZ/liblzma, and what implications does it have for open-source software security?\"\n",
    "question_2 = \"What specific vulnerabilities did the backdoor in the XZ Utils library exploit, and how did researchers identify its presence?\"\n",
    "question_3 = \"How did the Ivanti system vulnerabilities lead to the CISA intrusion, and what were the critical impacts on its systems and operations?\"\n",
    "question_4 = \"What new attack methods used by the APT41 subgroup Earth Freybug were discovered, and how do they target endpoint protection systems?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('frank_color')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e308f9137a7355e75444c23bfcca743591307fcf2fb23f92ada5abff93a4cda0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
